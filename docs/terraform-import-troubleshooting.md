# Terraform Import Errors Troubleshooting Guide

## Common Import Issues and Solutions

This guide addresses the specific "Request contains an invalid argument" errors encountered during Terraform deployment.

### Issue: Firebase Web App Import Error

**Error**: `Error when reading or editing FirebaseWebApp "projects/***/webApps/raseed-web-app": googleapi: Error 400: Request contains an invalid argument.`

**Root Cause**: Firebase Web App IDs are auto-generated by Google, not user-defined. The import ID in `imports.tf` was using a hardcoded name that doesn't exist.

**Solution**:

1. **Discover the actual app ID**:
   ```bash
   # Run the resource discovery script
   ./scripts/discover_resources.sh <PROJECT_ID> <REGION>
   
   # Or manually:
   gcloud firebase apps list --project=<PROJECT_ID>
   ```

2. **Update imports.tf with the real app ID**:
   ```hcl
   import {
     to = module.firebase.google_firebase_web_app.default
     id = "projects/PROJECT_ID/webApps/ACTUAL_APP_ID_FROM_GCLOUD"
   }
   ```

3. **If no Firebase app exists**, comment out the import block and let Terraform create it:
   ```hcl
   # import {
   #   to = module.firebase.google_firebase_web_app.default
   #   id = "projects/${var.project_id}/webApps/ACTUAL_APP_ID_HERE"
   # }
   ```

### Issue: Vertex AI Index Endpoint Import Error

**Error**: `Error when reading or editing VertexAIIndexEndpoint "projects/***/locations/us-central1/indexEndpoints/raseed-main-endpoint": googleapi: Error 400: Request contains an invalid argument.`

**Root Cause**: The IndexEndpoint resource name doesn't match the actual deployed resource name in Google Cloud.

**Solution**:

1. **Discover actual endpoint names**:
   ```bash
   # Using discovery script
   ./scripts/discover_resources.sh <PROJECT_ID> <REGION>
   
   # Or manually:
   gcloud ai index-endpoints list --region=us-central1 --project=<PROJECT_ID>
   ```

2. **Update imports.tf with correct endpoint name**:
   ```hcl
   import {
     to = module.vertex_ai.google_vertex_ai_index_endpoint.receipts_ep
     id = "projects/PROJECT_ID/locations/REGION/indexEndpoints/ACTUAL_ENDPOINT_NAME"
   }
   ```

3. **If no endpoint exists**, comment out the import and let quota cleanup run:
   ```bash
   # Clean up old endpoints first
   ./scripts/cleanup_quota.sh <PROJECT_ID> <REGION>
   ```

### Issue: Vertex AI Index Import Error

**Error**: `Error when reading or editing VertexAIIndex "projects/***/locations/us-central1/indexes/receipts-index": googleapi: Error 400: Request contains an invalid argument.`

**Root Cause**: Index resource names are auto-generated UUIDs, not the display names used in the import.

**Solution**:

1. **Discover actual index names**:
   ```bash
   # Get real index names
   gcloud ai indexes list --region=us-central1 --project=<PROJECT_ID> --format="table(name,displayName)"
   ```

2. **Update imports.tf with the actual index name**:
   ```hcl
   import {
     to = module.vertex_ai.google_vertex_ai_index.receipts
     id = "projects/PROJECT_ID/locations/REGION/indexes/ACTUAL_INDEX_UUID"
   }
   ```

3. **If no index exists**, remove the import and recreate:
   ```hcl
   # Comment out the import to create fresh
   # import {
   #   to = module.vertex_ai.google_vertex_ai_index.receipts
   #   id = "projects/${var.project_id}/locations/${var.region}/indexes/ACTUAL_INDEX_NAME_HERE"
   # }
   ```

## Automated Solutions

### Using the Resource Discovery Script

The `scripts/discover_resources.sh` script automatically finds real resource IDs:

```bash
# Run discovery
./scripts/discover_resources.sh <PROJECT_ID> <REGION>

# This will output actual import commands like:
# import {
#   to = module.firebase.google_firebase_web_app.default
#   id = "projects/my-project/webApps/1:123456789:web:abcdef123456"
# }
```

### Using the Enhanced Deployment Script

The updated `scripts/terraform_apply.sh` now includes:

1. **Resource Discovery**: Finds existing resources before import
2. **Quota Cleanup**: Removes conflicting old resources
3. **Safe Deployment**: Proceeds with corrected imports

```bash
# Run complete deployment with discovery and cleanup
./scripts/terraform_apply.sh infra/env/dev.tfvars
```

## Manual Resolution Steps

### Step 1: Clean Slate Approach

If imports are too complex, start fresh:

```bash
# 1. Clean up existing resources
./scripts/cleanup_quota.sh <PROJECT_ID> <REGION>

# 2. Comment out ALL import blocks in imports.tf
# Edit imports.tf and comment out problematic imports

# 3. Run terraform apply to create fresh resources
terraform -chdir=infra apply -var-file=env/dev.tfvars
```

### Step 2: Selective Import Approach

Import only resources that exist and are correctly named:

```bash
# 1. Discover what exists
./scripts/discover_resources.sh <PROJECT_ID> <REGION>

# 2. Update imports.tf with only the resources that were found

# 3. Test with terraform plan
terraform -chdir=infra plan -var-file=env/dev.tfvars

# 4. Apply when plan shows no errors
terraform -chdir=infra apply -var-file=env/dev.tfvars
```

### Step 3: Progressive Import Approach

Import resources one by one:

```bash
# 1. Comment out all imports in imports.tf

# 2. Uncomment and test one import at a time
terraform -chdir=infra plan -var-file=env/dev.tfvars

# 3. Fix any issues before proceeding to next import

# 4. Apply when all imports are working
terraform -chdir=infra apply -var-file=env/dev.tfvars
```

## Prevention Strategies

### 1. Use Resource Discovery Before Deployment

Always run resource discovery before modifying imports:

```bash
# Add to CI/CD pipeline
./scripts/discover_resources.sh $PROJECT_ID $REGION > discovered_imports.txt
```

### 2. Implement Stable Naming Patterns

- Use consistent, predictable resource names
- Avoid auto-generated IDs in critical resources
- Document naming conventions

### 3. Quota Management

- Run cleanup before each deployment
- Monitor quota usage regularly
- Use stable endpoint patterns for Vertex AI

### 4. Validation Pipeline

Add validation to deployment scripts:

```bash
# Validate imports before apply
terraform -chdir=infra plan -var-file=env/dev.tfvars

# Only proceed if plan succeeds
if [ $? -eq 0 ]; then
    terraform -chdir=infra apply -var-file=env/dev.tfvars
else
    echo "Plan failed, check imports"
    exit 1
fi
```

## Resource-Specific Notes

### Firebase Resources

- **Web App IDs**: Always auto-generated, format: `1:PROJECT_NUMBER:web:HASH`
- **Hosting Site IDs**: Usually `project-id-hosting` but can be custom
- **Project**: Uses project ID directly

### Vertex AI Resources

- **Index Names**: Auto-generated UUIDs like `projects/.../indexes/1234567890123456789`
- **Endpoint Names**: Auto-generated UUIDs like `projects/.../indexEndpoints/9876543210987654321`
- **Display Names**: User-defined but not used for resource identification

### Storage Buckets

- **Names**: Must be globally unique, usually `project-id-purpose`
- **Consistent**: Usually match Terraform configuration

### Service Accounts

- **Email Format**: `name@project-id.iam.gserviceaccount.com`
- **Predictable**: Usually match Terraform configuration

## Emergency Recovery

If deployment is completely broken:

1. **Nuclear Option**: Delete all resources and start fresh
   ```bash
   # WARNING: This deletes everything
   ./scripts/cleanup_quota.sh <PROJECT_ID> <REGION>
   
   # Comment out all imports
   # Run terraform apply to recreate everything
   ```

2. **Partial Recovery**: Import only essential resources
   ```bash
   # Keep storage buckets, service accounts
   # Recreate Firebase and Vertex AI resources
   ```

3. **State Management**: Fix Terraform state directly
   ```bash
   # Remove problematic resources from state
   terraform state rm module.firebase.google_firebase_web_app.default
   
   # Reimport with correct IDs
   terraform import module.firebase.google_firebase_web_app.default "projects/PROJECT/webApps/REAL_ID"
   ```

This comprehensive approach should resolve the import errors and prevent future occurrences.
